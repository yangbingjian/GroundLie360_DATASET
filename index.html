<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A New Dataset and Benchmark for Grounding Multimodal Misinformation</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f8f9fa;
            min-height: 100vh;
        }

        .container {
            width: 100%;
            max-width: 1400px;
            margin: 0 auto;
            padding: 60px 15%;
            background: transparent;
        }

        .header {
            text-align: center;
            margin-bottom: 60px;
            padding: 40px 0;
            background: transparent;
            color: #333;
        }

        .title {
            font-size: 3rem;
            font-weight: 630;
            margin-bottom: 30px;
            line-height: 1.2;
            color: #2c3e50;
        }

        .authors {
            font-size: 1.2rem;
            margin-bottom: 25px;
            color: #34495e;
        }

        .authors a {
            color: #3498db;
            text-decoration: none;
            transition: all 0.3s ease;
        }

        .authors a:hover {
            color: #2980b9;
            text-decoration: underline;
        }

        .affiliation {
            font-size: 1.1rem;
            margin-bottom: 30px;
            color: #7f8c8d;
        }

        .report-type {
            font-size: 2.2rem;
            font-weight: 600;
            margin-bottom: 30px;
            color: #2c3e50;
            display: inline-block;
        }

        .buttons {
            display: flex;
            justify-content: center;
            gap: 20px;
            margin-top: 40px;
            flex-wrap: wrap;
        }

        .btn {
            display: inline-flex;
            align-items: center;
            gap: 10px;
            padding: 15px 30px;
            background: #34495e;
            color: white;
            text-decoration: none;
            border-radius: 50px;
            font-weight: 600;
            transition: all 0.3s ease;
            font-size: 1rem;
        }

        .btn:hover {
            background: #2c3e50;
            transform: translateY(-3px);
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.2);
        }

        .overview-section {
            margin-bottom: 50px;
            padding: 40px 0;
            background: transparent;
        }

        .section {
            margin-bottom: 20px;
            padding: 30px 0;
            background: transparent;
            transition: transform 0.3s ease;
        }

        .section:hover {
            transform: none;
        }

        .section-title {
            font-size: 2.2rem;
            font-weight: 600;
            color: #2c3e50;
            margin-bottom: 30px;
            text-align: center;
            position: relative;
            padding-bottom: 15px;
        }

        .section-title::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 50%;
            transform: translateX(-50%);
            width: 60px;
            height: 3px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border-radius: 2px;
        }

        /* Supplementary section styling */
        .supplementary-section {
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            border-radius: 20px;
            padding: 40px;
            margin: 60px 0;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
        }

        .supplementary-title {
            font-size: 2.5rem;
            font-weight: 700;
            color: #2c3e50;
            margin-bottom: 40px;
            text-align: center;
            position: relative;
            padding-bottom: 20px;
        }

        .supplementary-title::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 50%;
            transform: translateX(-50%);
            width: 80px;
            height: 4px;
            background: linear-gradient(135deg, #8e44ad 0%, #3498db 100%);
            border-radius: 2px;
        }

        .subsection {
            margin-bottom: 50px;
            padding: 25px 0;
            border-left: 4px solid #3498db;
            padding-left: 30px;
            background: rgba(255, 255, 255, 0.7);
            border-radius: 0 15px 15px 0;
            margin-left: 20px;
        }

        .subsection-title {
            font-size: 1.8rem;
            font-weight: 600;
            color: #2c3e50;
            margin-bottom: 20px;
            position: relative;
        }

        .subsection-title::before {
            content: "▶";
            color: #3498db;
            margin-right: 10px;
            font-size: 1.2rem;
        }

        .subsubsection {
            margin: 30px 0;
            padding: 20px;
            background: rgba(255, 255, 255, 0.9);
            border-radius: 10px;
            border-left: 3px solid #8e44ad;
        }

        .subsubsection-title {
            font-size: 1.5rem;
            font-weight: 600;
            color: #34495e;
            margin-bottom: 15px;
        }

        .abstract-content {
            font-size: 1.15rem;
            line-height: 1.8;
            color: #34495e;
            text-align: justify;
            margin: 0 auto;
            max-width: 900px;
        }

        .supplementary-content {
            font-size: 1.1rem;
            line-height: 1.7;
            color: #34495e;
            text-align: justify;
        }

        .image-container {
            text-align: center;
            margin: 30px 0;
        }

        .image-container img {
            max-width: 85%;
            height: auto;
            border-radius: 15px;
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15);
            transition: all 0.4s ease;
        }

        .image-container img:hover {
            transform: scale(1.03);
            box-shadow: 0 15px 40px rgba(0, 0, 0, 0.2);
        }

        .image-caption {
            margin-top: 20px;
            font-style: italic;
            color: #566573;
            font-size: 1rem;
            line-height: 1.5;
            max-width: 800px;
            margin-left: auto;
            margin-right: auto;
            padding: 0 20px;
        }

        .highlight {
            font-weight: 700;
            color: #8e44ad;
            background: linear-gradient(135deg, #667eea, #764ba2);
            background-clip: text;
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .icon {
            width: 18px;
            height: 18px;
            fill: currentColor;
        }

        /* 双列布局用于Dataset Statistics */
        .stats-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 40px;
            margin-top: 30px;
        }

        .stats-item {
            text-align: center;
        }

        .stats-item img {
            max-width: 95%;
            height: auto;
            border-radius: 12px;
            box-shadow: 0 6px 20px rgba(0, 0, 0, 0.1);
        }

        /* List styling for supplementary overview */
        .overview-list {
            background: rgba(255, 255, 255, 0.8);
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
        }

        .overview-list ul {
            list-style: none;
            padding: 0;
        }

        .overview-list>ul>li {
            margin: 15px 0;
            padding: 10px 0;
            border-bottom: 1px solid #e0e0e0;
            font-weight: 600;
            color: #2c3e50;
        }

        .overview-list ol {
            margin: 10px 0 10px 20px;
            color: #34495e;
        }

        .overview-list ol li {
            margin: 8px 0;
            font-weight: 400;
        }

        /* 响应式设计 */
        @media (max-width: 1200px) {
            .container {
                padding: 40px 10%;
            }

            .title {
                font-size: 2.5rem;
            }
        }

        @media (max-width: 768px) {
            .container {
                padding: 30px 8%;
            }

            .title {
                font-size: 2rem;
            }

            .buttons {
                flex-direction: column;
                align-items: center;
            }

            .btn {
                width: 200px;
                justify-content: center;
            }

            .authors {
                font-size: 1rem;
            }

            .stats-grid {
                grid-template-columns: 1fr;
                gap: 30px;
            }

            .section {
                padding: 25px;
                margin-bottom: 40px;
            }

            .image-container img {
                max-width: 95%;
            }

            .supplementary-section {
                padding: 25px;
            }

            .subsection {
                margin-left: 10px;
                padding-left: 20px;
            }
        }

        /* 小屏设备的进一步优化 */
        @media (max-width: 480px) {
            .title {
                font-size: 1.8rem;
            }

            .section-title {
                font-size: 1.8rem;
            }

            .abstract-content {
                font-size: 1rem;
            }

            .supplementary-title {
                font-size: 2rem;
            }

            .subsection-title {
                font-size: 1.5rem;
            }
        }

        /* 添加平滑滚动 */
        html {
            scroll-behavior: smooth;
        }

        /* 添加加载动画 */
        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(30px);
            }

            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .section {
            animation: fadeInUp 0.6s ease-out;
        }
    </style>
</head>

<body>
    <div class="container">
        <div class="header">
            <h1 class="title">A New Dataset and Benchmark for Grounding Multimodal Misinformation</h1>

            <div class="authors">
                <a href="https://openreview.net/profile?id=~Bingjian_Yang1">Bingjian Yang</a><sup>1</sup>,
                <a href="https://openreview.net/profile?id=~Danni_Xu3">Danni Xu</a><sup>3</sup>,
                <a href="https://openreview.net/profile?id=~Kaipeng_Niu1">Xinyi Hu</a><sup>1</sup>,
                <a href="https://openreview.net/profile?id=~Wenxuan_Liu1">Wenxuan Liu</a><sup>2</sup>,
                <a href="https://openreview.net/profile?id=~Zheng_Wang14">Zheng Wang</a><sup>1</sup>,
                <a href="https://openreview.net/profile?id=~Mohan_Kankanhalli1">Mohan Kankanhalli</a><sup>3</sup>,

            </div>

            <div class="affiliation">
                <sup>1</sup>School of Computer Science, Wuhan University<br>
                <sup>2</sup>Peking University<br>
                <sup>3</sup>School of Computing, National University of Singapore<br>

            </div>

            <div class="report-type">Technical Report</div>

            <div class="buttons">
                <a href="https://github.com/yangbingjian/GroundLie360" class="btn">
                    <svg class="icon" viewBox="0 0 24 24">
                        <path
                            d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z" />
                    </svg>
                    Code
                </a>
                <a href="#" class="btn">
                    <svg class="icon" viewBox="0 0 24 24">
                        <path
                            d="M12,3C7.58,3 4,4.79 4,7C4,9.21 7.58,11 12,11C16.42,11 20,9.21 20,7C20,4.79 16.42,3 12,3M4,9V12C4,14.21 7.58,16 12,16C16.42,16 20,14.21 20,12V9C20,11.21 16.42,13 12,13C7.58,13 4,11.21 4,9M4,14V17C4,19.21 7.58,21 12,21C16.42,21 20,19.21 20,17V14C4,16.21 7.58,18 12,18C7.58,18 4,16.21 4,14Z" />
                    </svg>
                    Dataset
                </a>
            </div>
        </div>

        <div class="overview-section">
            <div class="image-container">
                <img src="dataset_overview.png" alt="GroundLie360 Dataset Overview">
                <div class="image-caption">
                    <strong>Overview.</strong> Our multi-modal benchmark contains 2,000+ fact-checked videos with fake
                    type and grounding annotations. Fake types include: (1) <span style="color: #2577C1;">False
                        Title</span>/<span style="color: #752995;">False Speech</span> - video title or spoken content
                    containing
                    demonstrably false claims; (2) <span style="color: #FB6A36;">Temporal Edit</span> - videos altered
                    to distort
                    event chronologies or fabricate deceptive narratives; (3) <span style="color: #FB2320;">CGI</span> -
                    digitally manipulated or generated synthetic media; (4) <span style="color: #1DA99E;">Contradictory
                        Content</span> - text-video semantic mismatches; and (5) <span
                        style="color: #6BC72B;">Unsupported Content</span>
                    - headlines lacking evidentiary support in video content. The dataset offers a unified benchmark for
                    fake content classification and localization.
                </div>
            </div>
        </div>

        <div class="section">
            <h2 class="section-title">Abstract</h2>
            The proliferation of online misinformation videos poses serious societal risks. Current datasets and
            detection methods primarily target binary classification or single-modality localization based on
            post-processed data, lacking the interpretability needed to counter persuasive misinformation. In this
            paper, we introduce the task of Grounding Multimodal Misinformation (GroundMM), which verifies
            multimodal content and localizes misleading segments across modalities. We present the first real-world
            dataset for this task, <span class="highlight">GroundLie360</span>, featuring a taxonomy of misinformation
            types, fine-grained
            annotations across text, speech, and visuals, and validation with Snopes evidence and annotator
            reasoning. We also propose a VLM-based, QA-driven baseline, <span class="highlight">FakeMark</span>, using
            single- and
            cross-modal cues for effective detection and grounding. Our experiments highlight the challenges of this
            task and lay a foundation for explainable multimodal misinformation detection.
        </div>
    </div>

    <div class="section">
        <h2 class="section-title">Dataset Pipeline</h2>
        <div class="image-container">
            <img src="dataset_pipeline.png" alt="Dataset Pipeline">
            <div class="image-caption">
                <strong>Construction pipeline of GroundLie360.</strong> It consists of three stages:
                (1) <strong>Video Collection</strong> harvesting videos from snopes.com; (2) <strong>Curation</strong>
                classifying videos into five categories: target videos (directly related to claims), original videos
                (pre-manipulation), debunking videos (refutations), evidence videos(supporting materials), and others;
                (3) <strong>Annotation</strong> labeling target videos with 6 fake types (False Title/Speech, Temporal
                Edits, CGI, and so on) and grounding information.
            </div>
        </div>
        <div class="image-container">
            <img src="dataset_comparision.png" alt="Dataset Comparison">
            <div class="image-caption">
                <strong>Comparison of Multimodal Fake News Datasets.</strong> Label Levels (L1: binary veracity
                classification. L2: fake types, L3: fake content grounding. M.A. (Manual Annotation), Annotation (Title:
                Textual headline, Speech: Transcript of verbal content, V.T.: Video Temporal localization, V.S.: Video
                Spatial localization). Range (RW-Gen: Real-World General Misinformation, DF: Deepfake, Tam: Tampering),
                Rat. (Explanation Rationale), and #Plat. (Number of platforms from which the video data was collected).
            </div>
        </div>
    </div>

    <div class="section">
        <h2 class="section-title">Method</h2>
        <div class="image-container">
            <img src="framework.png" alt="Method Overview">
            <div class="image-caption">
                <strong>Method Overview.</strong> (a) Overall pipeline; (b) Analysis Module, including text, video
                temporal/spatial, and cross-modality analyses; (c) Multimodal Classification and Grounding with a binary
                classifier, a multilabel classifier, and four localizers (false speech and contradictory content share
                designs with false title and unsupported types, respectively)
            </div>
        </div>
    </div>

    <div class="section">
        <h2 class="section-title">Results</h2>
        <div class="image-container">
            <img src="binary_result.png" alt="Experimental Results">
            <div class="image-caption">
                <strong>Binary classification results.</strong> <strong>Trad.</strong> and <strong>LLM</strong>
                represent for traditional video fake detection methods and LLM-based method.
            </div>
        </div>
        <div class="image-container">
            <img src="result2.png" alt="Experimental Results">
            <div class="image-caption">
                <strong>Subtype classification and text grounding performance across fake types.</strong>
            </div>
        </div>
        <div class="image-container">
            <img src="result3.png" alt="Experimental Results">
            <div class="image-caption">
                <strong>Performance on video grounding tasks.</strong> Temporal grounding is evaluated with Precision
                (Prec.), Recall (Rec.), and F1; spatial-temporal grounding with m_tIoU, m
                _vIoU, and vIoU@ thresholds. <strong>T.E.</strong> is temporal edit.
            </div>
        </div>
    </div>

    <!-- Supplementary Materials Section -->
    <div class="supplementary-section">
        <h2 class="supplementary-title">Supplementary Materials</h2>

        <div class="overview-list">
            <h3 style="color: #2c3e50; margin-bottom: 20px; font-size: 1.4rem;">Overview</h3>
            <p style="margin-bottom: 20px; color: #34495e;">This supplementary is organized as follows:</p>
            <ul>
                <li>Dataset Construction Details
                    <ol>
                        <li>Video Curation Process</li>
                        <li>Three-Level Annotation Process</li>
                        <li>Definition of Six Faking types</li>

                    </ol>
                </li>
                <li>Method Details
                    <ol>
                        <li>temporal edit localizer</li>
                    </ol>
                </li>

            </ul>
        </div>

        <div class="subsection" <h3 class="subsection-title">Dataset Construction Details</h3>
            </h3>

            <div class="subsubsection">
                <h4 class="subsubsection-title">Video Curation Process</h4>
                <div class="supplementary-content">
                    <p>
                        The dataset collection is followed by a crucial first step: video curation.
                        In addition to the target news videos, the auxiliary data in the dataset is manually categorized
                        into three types:
                        <em>debunking videos</em>, <em>original videos</em>, and <em>evidence videos</em>.
                        <em>Debunking videos</em> are typically produced by authoritative sources or fact-checking
                        organizations to explicitly refute misinformation or false claims presented in the target news
                        videos.
                        <em>Original videos</em> refer to unedited or primary-source footage that may have been
                        manipulated or misrepresented in the news content, providing a baseline for comparison and
                        authenticity verification.
                        <em>Evidence videos</em>, on the other hand, offer additional contextual or corroborative
                        information that indirectly supports the assessment of the video's authenticity and credibility.
                        Notably, only target videos are sent to the three-level annotation pipeline.
                    </p>


                </div>
                <div class="image-container">
                    <img src="auxiliary.png" alt="auxiliary data">
                    <div class="image-caption">
                        <strong>Examples of auxiliary videos</strong>

                    </div>
                </div>
            </div>

            <div class="subsubsection">
                <h4 class="subsubsection-title">Three-Level Annotation Process</h4>
                <div class="supplementary-content">
                    <p>
                        <a href="#3-level_annotation">Figure</a> presents the detailed pipelines of the 3-level annotation scheme. Firstly, the
                        filtered target video is automatically assigned a veracity label based on the rating provided by
                        the fact-checking website Snopes. For samples labeled as fake, annotators are required to
                        separately assess the presence of each of the six predefined types, thereby performing a
                        second-level multilabel classification. To implement fine-grained third-level annotation, each
                        identified fake label is further grounded to specific locations within the corresponding
                        modality. The grounding process varies depending on the modality and the type of fake content:
                        (1) For textual content (e.g., video headlines, speech), grounding refers to highlighting the
                        specific phrase or entire sentence that contains false or misleading information. (2) For video
                        content, grounding is typically temporal for the temporal edit type, marking the timestamps of
                        deceptive editing operations. In the case of CGI-based falsification, spatial grounding is also
                        applied to indicate the manipulated area within the video frame. (3) For cross-modal
                        inconsistencies, grounding involves locating contradictions or unsupported claims in the text
                        relative to the video. <strong>Notably, candidate timestamps for the temporal edit type are
                            discrete, comprising start timestamp, end timestamp, and scene transition timestamp, which
                            are uniformly mapped to a binary vector indicating the locations of deceptive edit
                            operations.</strong>
                    </p>

                </div>
                <div class="image-container" id="3-level_annotation">
                    <img src="3-level_annotation.png" alt="3-level Annotation">
                    <div class="image-caption">
                        <strong>An example pipeline of 3-level annotation scheme.</strong>
                    </div>
                </div>
            </div>

            <div class="subsubsection">
                <h4 class="subsubsection-title">Definition of Six Faking types</h4>
                <div class="supplementary-content">
                    <p>To ensure consistency in classification and grounding across different annotators, we provide
                        detailed definitions for each sub-label. </p>
                </div>
                <div class="image-container">
                    <img src="definition.png" alt="Definition of six types">
                    <div class="image-caption">
                        <strong>Definitions of sub-labels used in the proposed dataset.</strong>
                    </div>
                </div>
            </div>
        </div>

        <div class="subsection">
            <h3 class="subsection-title">Method Details</h3>
            <div class="subsubsection">
                <h4 class="subsubsection-title">Temporal Edit Localizer</h4>
                <div class="supplementary-content">
                    <p>
                        For the sake of brevity, the temporal edit localizer is presented in a simplified form in the
                        main text.
                        <strong>In practice, in addition to scene transitions, the candidate grounding element also
                            includes the start and end of video as shown in <a href="#fig-TEL">Figure</a>.</strong>
                        Aligned with the annotation process, the output of the temporal edit localizer is also a binary
                        vector indicating the locations of deceptive edit operations.
                    </p>
                </div>
                <div class="image-container" id="fig-TEL">
                    <img src="TEL.png" alt="temporal edit localizer">
                    <div class="image-caption">
                        <strong>The actual implementation of temporal edit localizer.</strong> The VLM first checks whether the start or end of the video has been deceptively edited. It then evaluates transitions between scenes to detect malicious edits. 
                    </div>
                </div>
            </div>
        </div>


    </div>
    </div>
</body>

</html>